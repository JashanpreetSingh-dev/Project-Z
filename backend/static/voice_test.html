<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Receptionist Test</title>
    <style>
        * {
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #0a0a0a;
            color: #e5e5e5;
        }
        h1 {
            color: #fff;
            margin-bottom: 10px;
        }
        .subtitle {
            color: #888;
            margin-bottom: 30px;
        }
        .controls {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.2s;
        }
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        #startBtn {
            background: #22c55e;
            color: white;
        }
        #startBtn:hover:not(:disabled) {
            background: #16a34a;
        }
        #stopBtn {
            background: #ef4444;
            color: white;
        }
        #stopBtn:hover:not(:disabled) {
            background: #dc2626;
        }
        #textBtn {
            background: #3b82f6;
            color: white;
        }
        #textBtn:hover:not(:disabled) {
            background: #2563eb;
        }
        .status {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 15px;
            background: #1a1a1a;
            border-radius: 8px;
            margin-bottom: 20px;
        }
        .status-indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #666;
        }
        .status-indicator.connected {
            background: #22c55e;
        }
        .status-indicator.listening {
            background: #3b82f6;
            animation: pulse 1s infinite;
        }
        .status-indicator.speaking {
            background: #f59e0b;
            animation: pulse 0.5s infinite;
        }
        .status-indicator.processing {
            background: #8b5cf6;
            animation: pulse 0.3s infinite;
        }
        .status-indicator.error {
            background: #ef4444;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .transcript {
            background: #1a1a1a;
            border-radius: 8px;
            padding: 20px;
            min-height: 300px;
            max-height: 500px;
            overflow-y: auto;
        }
        .message {
            margin-bottom: 15px;
            padding: 10px 15px;
            border-radius: 8px;
            max-width: 80%;
        }
        .message.user {
            background: #1e3a5f;
            margin-left: auto;
            text-align: right;
        }
        .message.assistant {
            background: #2d2d2d;
        }
        .message .role {
            font-size: 12px;
            color: #888;
            margin-bottom: 5px;
        }
        .text-input {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }
        .text-input input {
            flex: 1;
            padding: 12px;
            font-size: 16px;
            border: 1px solid #333;
            border-radius: 8px;
            background: #1a1a1a;
            color: #e5e5e5;
        }
        .text-input input:focus {
            outline: none;
            border-color: #3b82f6;
        }
        .info {
            background: #1e293b;
            border: 1px solid #334155;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 20px;
            font-size: 14px;
        }
        .info code {
            background: #334155;
            padding: 2px 6px;
            border-radius: 4px;
        }
    </style>
</head>
<body>
    <h1>Voice Receptionist Test</h1>
    <p class="subtitle">OpenAI Realtime API Integration</p>

    <div class="info">
        <strong>Instructions:</strong> Click "Start" to begin a voice session. 
        Speak into your microphone or use the text input below for testing.
        Audio format: <code>16kHz, 16-bit PCM, mono</code>
    </div>

    <div class="controls">
        <button id="startBtn" onclick="startSession()">üé§ Start Voice</button>
        <button id="stopBtn" onclick="stopSession()" disabled>‚èπÔ∏è Stop</button>
    </div>

    <div class="text-input">
        <input type="text" id="textInput" placeholder="Or type a message..." onkeypress="handleKeyPress(event)">
        <button id="textBtn" onclick="sendText()" disabled>Send</button>
    </div>

    <div class="status">
        <div class="status-indicator" id="statusIndicator"></div>
        <span id="statusText">Disconnected</span>
    </div>

    <div class="transcript" id="transcript">
        <p style="color: #666; text-align: center;">Conversation will appear here...</p>
    </div>

    <script>
        let websocket = null;
        let audioContext = null;
        let mediaStream = null;
        let audioWorklet = null;
        let isRecording = false;

        // Audio playback queue
        let audioQueue = [];
        let isPlaying = false;

        function updateStatus(state, text) {
            const indicator = document.getElementById('statusIndicator');
            const statusText = document.getElementById('statusText');
            
            indicator.className = 'status-indicator ' + state;
            statusText.textContent = text;
        }

        function addMessage(role, text) {
            const transcript = document.getElementById('transcript');
            
            // Remove placeholder if present
            if (transcript.querySelector('p[style]')) {
                transcript.innerHTML = '';
            }
            
            const div = document.createElement('div');
            div.className = 'message ' + role;
            div.innerHTML = `
                <div class="role">${role === 'user' ? 'You' : 'AI Assistant'}</div>
                <div class="text">${text}</div>
            `;
            transcript.appendChild(div);
            transcript.scrollTop = transcript.scrollHeight;
        }

        async function startSession() {
            try {
                // Get microphone access
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                    }
                });

                // Create audio context
                audioContext = new AudioContext({ sampleRate: 16000 });
                
                // Connect to WebSocket
                const wsUrl = `ws://${window.location.host}/api/voice/stream`;
                websocket = new WebSocket(wsUrl);
                websocket.binaryType = 'arraybuffer';

                websocket.onopen = () => {
                    updateStatus('connected', 'Connected - Speak now');
                    document.getElementById('startBtn').disabled = true;
                    document.getElementById('stopBtn').disabled = false;
                    document.getElementById('textBtn').disabled = false;
                    startRecording();
                };

                websocket.onmessage = async (event) => {
                    if (event.data instanceof ArrayBuffer) {
                        // Audio data from AI
                        await playAudio(event.data);
                    } else {
                        // JSON message
                        const data = JSON.parse(event.data);
                        handleMessage(data);
                    }
                };

                websocket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    updateStatus('error', 'Connection error');
                };

                websocket.onclose = () => {
                    updateStatus('', 'Disconnected');
                    stopRecording();
                    document.getElementById('startBtn').disabled = false;
                    document.getElementById('stopBtn').disabled = true;
                    document.getElementById('textBtn').disabled = true;
                };

            } catch (error) {
                console.error('Error starting session:', error);
                updateStatus('error', 'Error: ' + error.message);
            }
        }

        function handleMessage(data) {
            switch (data.type) {
                case 'transcript':
                    addMessage(data.role, data.text);
                    break;
                case 'state':
                    handleStateChange(data.state);
                    break;
                case 'error':
                    console.error('Server error:', data.message);
                    updateStatus('error', 'Error: ' + data.message);
                    break;
            }
        }

        function handleStateChange(state) {
            const stateLabels = {
                'idle': 'Idle',
                'connecting': 'Connecting...',
                'listening': 'Listening...',
                'processing': 'Processing...',
                'speaking': 'AI Speaking...',
                'ended': 'Session Ended',
                'error': 'Error',
            };
            updateStatus(state, stateLabels[state] || state);
        }

        async function startRecording() {
            if (!audioContext || !mediaStream) return;

            isRecording = true;

            // Create audio processing
            const source = audioContext.createMediaStreamSource(mediaStream);
            
            // Use ScriptProcessor for simplicity (AudioWorklet would be better for production)
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = (e) => {
                if (!isRecording || !websocket || websocket.readyState !== WebSocket.OPEN) {
                    return;
                }

                const inputData = e.inputBuffer.getChannelData(0);
                
                // Convert Float32 to Int16
                const pcm16 = new Int16Array(inputData.length);
                for (let i = 0; i < inputData.length; i++) {
                    const s = Math.max(-1, Math.min(1, inputData[i]));
                    pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }

                // Send to WebSocket
                websocket.send(pcm16.buffer);
            };

            source.connect(processor);
            processor.connect(audioContext.destination);
        }

        function stopRecording() {
            isRecording = false;
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
        }

        async function playAudio(arrayBuffer) {
            // Queue audio for playback
            audioQueue.push(arrayBuffer);
            
            if (!isPlaying) {
                playNextAudio();
            }
        }

        async function playNextAudio() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const arrayBuffer = audioQueue.shift();

            try {
                // Create audio context for playback (24kHz from OpenAI)
                const playbackContext = new AudioContext({ sampleRate: 24000 });
                
                // Convert PCM16 to Float32
                const pcm16 = new Int16Array(arrayBuffer);
                const float32 = new Float32Array(pcm16.length);
                for (let i = 0; i < pcm16.length; i++) {
                    float32[i] = pcm16[i] / 32768;
                }

                // Create audio buffer
                const audioBuffer = playbackContext.createBuffer(1, float32.length, 24000);
                audioBuffer.copyToChannel(float32, 0);

                // Play
                const source = playbackContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(playbackContext.destination);
                source.onended = () => {
                    playbackContext.close();
                    playNextAudio();
                };
                source.start();

            } catch (error) {
                console.error('Error playing audio:', error);
                playNextAudio();
            }
        }

        function stopSession() {
            if (websocket && websocket.readyState === WebSocket.OPEN) {
                websocket.send(JSON.stringify({ type: 'end' }));
                websocket.close();
            }
            stopRecording();
        }

        function sendText() {
            const input = document.getElementById('textInput');
            const text = input.value.trim();
            
            if (text && websocket && websocket.readyState === WebSocket.OPEN) {
                websocket.send(JSON.stringify({ type: 'text', text: text }));
                addMessage('user', text);
                input.value = '';
            }
        }

        function handleKeyPress(event) {
            if (event.key === 'Enter') {
                sendText();
            }
        }

        // Cleanup on page unload
        window.addEventListener('beforeunload', stopSession);
    </script>
</body>
</html>
